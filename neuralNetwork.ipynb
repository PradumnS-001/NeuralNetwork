{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ee4a8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "045ce3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining necessary functions\n",
    "\n",
    "def softmax(z):\n",
    "    z = np.clip(z, -100, 100)\n",
    "    z = z - np.max(z)\n",
    "    exps = np.exp(z)\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "def leaky_relu(x):\n",
    "    for  i, n in enumerate(x):\n",
    "        for k,_ in enumerate(n):\n",
    "            x[i,k] = np.maximum(0.01 * x[i,k], x[i,k])\n",
    "            \n",
    "    return x\n",
    "\n",
    "def D_leaky_relu(x):\n",
    "    for  i, n in enumerate(x):\n",
    "        for k,_ in enumerate(n):\n",
    "            x[i,k] = 0.01 if x[i,k] <= 0 else 1\n",
    "            \n",
    "    return x\n",
    "\n",
    "def row(m):\n",
    "    m = m.reshape(-1,1)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e27e7e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparamaters, weights, biases, dataset imports\n",
    "\n",
    "W1 = np.random.randn(16, 784) * np.sqrt(2 / ((1 + 0.01**2) * 784))\n",
    "W2 = np.random.randn(16, 16) * np.sqrt(2 / ((1 + 0.01**2) * 16))\n",
    "W3 = np.random.randn(10, 16) * np.sqrt(1 / (16 + 10))\n",
    "b1 = np.zeros((16,1))\n",
    "b2 = np.zeros((16,1))\n",
    "b3 = np.zeros((10,1))\n",
    "\n",
    "eta = 0.05\n",
    "\n",
    "train_df = pd.read_csv(\"CSVs\\\\mnist_train.csv\").sample(frac=1).reset_index(drop=True)\n",
    "test_df = pd.read_csv(\"CSVs\\\\mnist_test.csv\").sample(frac=1).reset_index(drop=True)\n",
    "train_img = np.array(train_df.iloc[:,1:]) / 255\n",
    "train_label = np.array(train_df.iloc[:,0])\n",
    "test_img = np.array(test_df.iloc[:,1:]) / 255\n",
    "test_label = np.array(test_df.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f316cf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ForwardPass(neuralInput):\n",
    "    neuralInput = row(neuralInput)\n",
    "    z1 = W1 @ neuralInput + b1\n",
    "    a1 = leaky_relu(z1)\n",
    "    z2 = W2 @ a1 + b2\n",
    "    a2 = leaky_relu(z2)\n",
    "    neuralOutput = softmax(W3 @ a2 + b3)\n",
    "    return [neuralOutput, z2, z1]\n",
    "\n",
    "dW3 = np.zeros(np.shape(W3))\n",
    "dW2 = np.zeros(np.shape(W2))\n",
    "dW1 = np.zeros(np.shape(W1))\n",
    "db3 = np.zeros(np.shape(b3))\n",
    "db2 = np.zeros(np.shape(b2))\n",
    "db1 = np.zeros(np.shape(b1))\n",
    "\n",
    "def backPass(img, label):\n",
    "    \n",
    "    global dW3,dW2,dW1,db1,db2,db3\n",
    "    y = np.zeros((10,1))\n",
    "    y[label] = 1\n",
    "    \n",
    "    NO, z2, z1 = ForwardPass(img)\n",
    "    \n",
    "    dW3 += (NO - y)@leaky_relu(z2).T\n",
    "    db3 += (NO - y)\n",
    "    da2 = row(np.sum((NO - y)*W3, axis=0))\n",
    "    dW2 += da2*D_leaky_relu(z2)@leaky_relu(z1).T\n",
    "    db2 += da2*D_leaky_relu(z2)\n",
    "    da1 = row(np.sum(da2*D_leaky_relu(z2)*W2, axis=0))\n",
    "    dW1 += (da1*D_leaky_relu(z1))@img.reshape(1, -1)\n",
    "    db1 += da1*D_leaky_relu(z1)\n",
    "    \n",
    "def backProp(img_batch, lable_batch):\n",
    "    \n",
    "    global dW3,dW2,dW1,db1,db2,db3\n",
    "    dW3 *= 0\n",
    "    dW2 *= 0\n",
    "    dW1 *= 0\n",
    "    db3 *= 0\n",
    "    db2 *= 0\n",
    "    db1 *= 0\n",
    "    \n",
    "    global W1,W2,W3,b1,b2,b3\n",
    "    \n",
    "    for img, label in zip(img_batch, lable_batch):\n",
    "        backPass(img, label)\n",
    "        \n",
    "    max_grad_norm = 1.0\n",
    "    grads = [dW1, dW2, dW3, db1, db2, db3]\n",
    "    for grad in grads:\n",
    "        np.clip(grad, -max_grad_norm, max_grad_norm, out=grad)\n",
    "        \n",
    "    W1 -= eta * dW1 / 120\n",
    "    W2 -= eta * dW2 / 120\n",
    "    W3 -= eta * dW3 / 120\n",
    "    b1 -= eta * db1 / 120\n",
    "    b2 -= eta * db2 / 120\n",
    "    b3 -= eta * db3 / 120\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc9591d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch():\n",
    "    for i in range(500):\n",
    "        backProp(train_img[i*120:i*120+120,:], train_label[i*120:i*120+120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43ac4262",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_guesses = 0\n",
    "def test_batch(imgs, labels):\n",
    "    global correct_guesses\n",
    "    for img,label in zip(imgs,labels):\n",
    "        if np.argmax(ForwardPass(row(img))[0]) == label:\n",
    "            correct_guesses += 1\n",
    "            \n",
    "def accuracy():\n",
    "    global correct_guesses\n",
    "    for i in range(50):\n",
    "        test_batch(test_img[i*200:i*200+200,:],test_label[i*200:i*200+200])\n",
    "        \n",
    "    print(\"Test accuracy:\",(correct_guesses/100))\n",
    "    correct_guesses = 0\n",
    "    for i in range(300):\n",
    "        test_batch(train_img[i*200:i*200+200,:],train_label[i*200:i*200+200])\n",
    "        \n",
    "    print(\"Train accuracy:\",(correct_guesses/600))\n",
    "    correct_guesses = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a41c015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 81.66\n",
      "Train accuracy: 81.29833333333333\n",
      "Test accuracy: 88.44\n",
      "Train accuracy: 87.63166666666666\n",
      "Test accuracy: 89.94\n",
      "Train accuracy: 89.315\n",
      "Test accuracy: 90.58\n",
      "Train accuracy: 90.145\n",
      "Test accuracy: 91.34\n",
      "Train accuracy: 90.795\n",
      "Test accuracy: 91.79\n",
      "Train accuracy: 91.20333333333333\n",
      "Test accuracy: 92.04\n",
      "Train accuracy: 91.57333333333334\n",
      "Test accuracy: 92.23\n",
      "Train accuracy: 91.84833333333333\n",
      "Test accuracy: 92.42\n",
      "Train accuracy: 92.16166666666666\n",
      "Test accuracy: 92.53\n",
      "Train accuracy: 92.37666666666667\n",
      "Test accuracy: 92.73\n",
      "Train accuracy: 92.56333333333333\n",
      "Test accuracy: 92.94\n",
      "Train accuracy: 92.705\n",
      "Test accuracy: 92.98\n",
      "Train accuracy: 92.84333333333333\n",
      "Test accuracy: 93.15\n",
      "Train accuracy: 92.975\n",
      "Test accuracy: 93.19\n",
      "Train accuracy: 93.11\n",
      "Test accuracy: 93.22\n",
      "Train accuracy: 93.215\n",
      "Test accuracy: 93.27\n",
      "Train accuracy: 93.335\n",
      "Test accuracy: 93.32\n",
      "Train accuracy: 93.415\n"
     ]
    }
   ],
   "source": [
    "for i in range(18):\n",
    "    epoch()\n",
    "    accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "baa2e7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = pd.DataFrame(W1)\n",
    "b1 = pd.DataFrame(b1)\n",
    "W2 = pd.DataFrame(W2)\n",
    "b2 = pd.DataFrame(b2)\n",
    "W3 = pd.DataFrame(W3)\n",
    "b3 = pd.DataFrame(b3)\n",
    "\n",
    "W1.to_csv(\"CSVs\\\\W1.csv\", index=False, header=False)\n",
    "b1.to_csv(\"CSVs\\\\b1.csv\", index=False, header=False)\n",
    "W2.to_csv(\"CSVs\\\\W2.csv\", index=False, header=False)\n",
    "b2.to_csv(\"CSVs\\\\b2.csv\", index=False, header=False)\n",
    "W3.to_csv(\"CSVs\\\\W3.csv\", index=False, header=False)\n",
    "b3.to_csv(\"CSVs\\\\b3.csv\", index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
